{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22a2bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from kafka import KafkaConsumer\n",
    "from avro.io import DatumReader, BinaryDecoder\n",
    "from avro.schema import parse\n",
    "from io import BytesIO\n",
    "\n",
    "# Ensure transaction-metadata package path is available (project-local)\n",
    "transaction_metadata_path = \"/Users/manojitroy/flink-practice/transaction-project/transaction-metadata\"\n",
    "if transaction_metadata_path not in sys.path:\n",
    "    sys.path.insert(0, transaction_metadata_path)\n",
    "\n",
    "from transaction_metadata.loader import get_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11412431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the transaction schema (returns a python dict/object) and parse into Avro schema object\n",
    "transaction_schema = get_schema(\"transaction\", version=\"v1\")\n",
    "schema = parse(json.dumps(transaction_schema))\n",
    "print(\"Schema loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dda8bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_avro(binary_message: bytes, avro_schema):\n",
    "    \"\"\"Decode Avro binary `binary_message` using a parsed Avro schema object.\"\"\"\n",
    "    if binary_message is None:\n",
    "        return None\n",
    "    try:\n",
    "        bytes_reader = BytesIO(binary_message)\n",
    "        decoder = BinaryDecoder(bytes_reader)\n",
    "        reader = DatumReader(avro_schema)\n",
    "        return reader.read(decoder)\n",
    "    except Exception as e:\n",
    "        # If decoding fails, try to fall back to JSON parse (some messages may be JSON)\n",
    "        try:\n",
    "            text = binary_message.decode('utf-8')\n",
    "            return json.loads(text)\n",
    "        except Exception:\n",
    "            print('Failed to decode message as Avro or JSON:', e)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14b9eb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer created. You can call `consume_messages()` to fetch and print messages.\n"
     ]
    }
   ],
   "source": [
    "# Create a consumer that automatically decodes Avro messages using `decode_avro`\n",
    "consumer = KafkaConsumer(\n",
    "    \"transactions\",\n",
    "    bootstrap_servers=\"localhost:9092\",\n",
    "    security_protocol=\"PLAINTEXT\",\n",
    "    auto_offset_reset=\"earliest\",\n",
    "    enable_auto_commit=True,\n",
    "    consumer_timeout_ms=1000,  # short timeout so notebook cells return\n",
    "    value_deserializer=lambda m: decode_avro(m, schema),\n",
    ")\n",
    "\n",
    "print(\"Consumer created. You can call `consume_messages()` to fetch and print messages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95365f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_messages(max_messages=None, timeout_seconds=10):\n",
    "    \"\"\"Consume messages from the already-created `consumer`, decode and pretty print them.\"\"\"\n",
    "    received = 0\n",
    "    try:\n",
    "        for message in consumer:\n",
    "            data = message.value\n",
    "            print('---')\n",
    "            print(f'Partition: {message.partition}, Offset: {message.offset}')\n",
    "            if data is None:\n",
    "                print('Received empty or undecodable message')\n",
    "            else:\n",
    "                # Print a compact summary then the full payload\n",
    "                print('Transaction ID:', data.get('transaction_id'))\n",
    "                print('User ID:', data.get('user_id'))\n",
    "                print('Amount:', data.get('amount'))\n",
    "                print('Location:', data.get('location'))\n",
    "                print('Full payload:\\n', json.dumps(data, indent=2))\n",
    "            received += 1\n",
    "            if max_messages is not None and received >= max_messages:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        print('Stopped by user')\n",
    "    finally:\n",
    "        # do not close the consumer here if you want to reuse it, but provide a helper\n",
    "        pass\n",
    "\n",
    "def close_consumer():\n",
    "    consumer.close()\n",
    "    print('Consumer closed')\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# consume_messages(max_messages=5)\n",
    "# close_consumer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec030d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2837.96s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Stopped by user\n",
      "Stopped by user\n"
     ]
    }
   ],
   "source": [
    "# Ensure the lz4 codec library is installed in the notebook environment so Avro can decode lz4-compressed payloads.\n",
    "# Use the %pip magic to install into the running kernel environment.\n",
    "%pip install --quiet lz4\n",
    "\n",
    "# Import lz4 so the codec is available to the Avro runtime\n",
    "import lz4\n",
    "\n",
    "# Now consume messages (will work for lz4-compressed Avro messages)\n",
    "consume_messages(max_messages=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7ee16",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedCodecError",
     "evalue": "UnsupportedCodecError: Libraries for lz4 compression codec not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnsupportedCodecError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconsumer\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flink-practice/transaction-project/virutual-env/lib/python3.14/site-packages/kafka/consumer/group.py:1188\u001b[39m, in \u001b[36mKafkaConsumer.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1186\u001b[39m     \u001b[38;5;28mself\u001b[39m._iterator = \u001b[38;5;28mself\u001b[39m._message_generator_v2()\n\u001b[32m   1187\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1188\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   1190\u001b[39m     \u001b[38;5;28mself\u001b[39m._iterator = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flink-practice/transaction-project/virutual-env/lib/python3.14/site-packages/kafka/consumer/group.py:1160\u001b[39m, in \u001b[36mKafkaConsumer._message_generator_v2\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1158\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1159\u001b[39m     timeout_ms = \u001b[32m1000\u001b[39m * \u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m._consumer_timeout - time.time())\n\u001b[32m-> \u001b[39m\u001b[32m1160\u001b[39m     record_map = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1161\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six.iteritems(record_map):\n\u001b[32m   1162\u001b[39m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[32m   1163\u001b[39m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[32m   1164\u001b[39m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[32m   1165\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[32m   1166\u001b[39m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[32m   1167\u001b[39m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[32m   1168\u001b[39m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[32m   1169\u001b[39m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flink-practice/transaction-project/virutual-env/lib/python3.14/site-packages/kafka/consumer/group.py:684\u001b[39m, in \u001b[36mKafkaConsumer.poll\u001b[39m\u001b[34m(self, timeout_ms, max_records, update_offsets)\u001b[39m\n\u001b[32m    682\u001b[39m timer = Timer(timeout_ms)\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._closed:\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m     records = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    685\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[32m    686\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flink-practice/transaction-project/virutual-env/lib/python3.14/site-packages/kafka/consumer/group.py:738\u001b[39m, in \u001b[36mKafkaConsumer._poll_once\u001b[39m\u001b[34m(self, timer, max_records, update_offsets)\u001b[39m\n\u001b[32m    735\u001b[39m     log.debug(\u001b[33m'\u001b[39m\u001b[33mpoll: coordinator needs rejoin; returning early\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m records, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetched_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flink-practice/transaction-project/virutual-env/lib/python3.14/site-packages/kafka/consumer/fetcher.py:351\u001b[39m, in \u001b[36mFetcher.fetched_records\u001b[39m\u001b[34m(self, max_records, update_offsets)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m drained:\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    352\u001b[39m     \u001b[38;5;66;03m# To be thrown in the next call of this method\u001b[39;00m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28mself\u001b[39m._next_in_line_exception_metadata = ExceptionMetadata(fetched_partition, fetched_offset, e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flink-practice/transaction-project/virutual-env/lib/python3.14/site-packages/kafka/consumer/fetcher.py:345\u001b[39m, in \u001b[36mFetcher.fetched_records\u001b[39m\u001b[34m(self, max_records, update_offsets)\u001b[39m\n\u001b[32m    343\u001b[39m             fetched_partition = \u001b[38;5;28mself\u001b[39m._next_partition_records.topic_partition\n\u001b[32m    344\u001b[39m             fetched_offset = \u001b[38;5;28mself\u001b[39m._next_partition_records.next_fetch_offset\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m             records_remaining -= \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_append\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_partition_records\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m                                              \u001b[49m\u001b[43mrecords_remaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m                                              \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m drained:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flink-practice/transaction-project/virutual-env/lib/python3.14/site-packages/kafka/consumer/fetcher.py:379\u001b[39m, in \u001b[36mFetcher._append\u001b[39m\u001b[34m(self, drained, part, max_records, update_offsets)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m part.next_fetch_offset == position.offset:\n\u001b[32m    377\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mReturning fetched records at offset \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m for assigned\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    378\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33m partition \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, position.offset, tp)\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     part_records = \u001b[43mpart\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m     \u001b[38;5;66;03m# list.extend([]) is a noop, but because drained is a defaultdict\u001b[39;00m\n\u001b[32m    381\u001b[39m     \u001b[38;5;66;03m# we should avoid initializing the default list unless there are records\u001b[39;00m\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m part_records:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flink-practice/transaction-project/virutual-env/lib/python3.14/site-packages/kafka/consumer/fetcher.py:977\u001b[39m, in \u001b[36mFetcher.PartitionRecords.take\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    976\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m records:\n\u001b[32m--> \u001b[39m\u001b[32m977\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    978\u001b[39m     \u001b[38;5;66;03m# To be thrown in the next call of this method\u001b[39;00m\n\u001b[32m    979\u001b[39m     \u001b[38;5;28mself\u001b[39m._next_inline_exception = e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flink-practice/transaction-project/virutual-env/lib/python3.14/site-packages/kafka/consumer/fetcher.py:974\u001b[39m, in \u001b[36mFetcher.PartitionRecords.take\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    971\u001b[39m records = []\n\u001b[32m    972\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    973\u001b[39m     \u001b[38;5;66;03m# Note that records.extend(iter) will extend partially when exception raised mid-stream\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m     \u001b[43mrecords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecord_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    976\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m records:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flink-practice/transaction-project/virutual-env/lib/python3.14/site-packages/kafka/consumer/fetcher.py:1026\u001b[39m, in \u001b[36mFetcher.PartitionRecords._unpack_records\u001b[39m\u001b[34m(self, tp, records, key_deserializer, value_deserializer)\u001b[39m\n\u001b[32m   1023\u001b[39m         batch = records.next_batch()\n\u001b[32m   1024\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_crcs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_crc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mraise\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mErrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCorruptRecordError\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRecord for partition \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m at offset \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m failed crc check\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtopic_partition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flink-practice/transaction-project/virutual-env/lib/python3.14/site-packages/kafka/record/default_records.py:335\u001b[39m, in \u001b[36mDefaultRecordBatch.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_uncompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flink-practice/transaction-project/virutual-env/lib/python3.14/site-packages/kafka/record/default_records.py:238\u001b[39m, in \u001b[36mDefaultRecordBatch._maybe_uncompress\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    236\u001b[39m compression_type = \u001b[38;5;28mself\u001b[39m.compression_type\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression_type != \u001b[38;5;28mself\u001b[39m.CODEC_NONE:\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_assert_has_codec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompression_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m     data = \u001b[38;5;28mmemoryview\u001b[39m(\u001b[38;5;28mself\u001b[39m._buffer)[\u001b[38;5;28mself\u001b[39m._pos:]\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression_type == \u001b[38;5;28mself\u001b[39m.CODEC_GZIP:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flink-practice/transaction-project/virutual-env/lib/python3.14/site-packages/kafka/record/default_records.py:123\u001b[39m, in \u001b[36mDefaultRecordBase._assert_has_codec\u001b[39m\u001b[34m(self, compression_type)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnsupportedCodecError(\u001b[33m\"\u001b[39m\u001b[33mUnrecognized compression type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (compression_type,))\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m checker():\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnsupportedCodecError(\n\u001b[32m    124\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLibraries for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m compression codec not found\u001b[39m\u001b[33m\"\u001b[39m.format(name))\n",
      "\u001b[31mUnsupportedCodecError\u001b[39m: UnsupportedCodecError: Libraries for lz4 compression codec not found"
     ]
    }
   ],
   "source": [
    "# End of consumer cells. Use `consume_messages()` to fetch and inspect messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e95f08af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method KafkaConsumer._message_generator_v2 of <kafka.consumer.group.KafkaConsumer object at 0x10936b8c0>>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395bed26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transaction Simulator Kernel",
   "language": "python",
   "name": "txn-sim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
